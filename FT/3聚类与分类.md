# 聚类

## K-means和K-medoids

Mean($\mu_i$)：聚类的中心

Medoid($x_k$)：聚类的中心数据点(到类内每个数据点的距离之和最小)

K-means特点:

* 简单快速
* 聚类结果容易受到起始点影响
* 聚类结果在向量空间为球状(凸集)
* 聚类结果容易受噪声(脏数据)影响



arg min f(x)：当f(x)取最小值时，x的取值



KMeans

1. 在样本中随机选取k个样本点充当各个簇的中心点$\{\mu_{1},\mu_{2},...,\mu_{k}\}$
2. 计算所有样本点与各个簇中心之间的距离$dist(x^{(i)},\mu_{j})$，然后把样本点划入最近的簇中$x^{(i)}\in{\mu_{nearest}}$
3. 根据簇中已有的样本点，重新计算簇中心$\mu_{i}:=\frac{1}{|C_{i}|}\sum_{x\in{C{i}}}x$



如何计算中心数据点

* 方法一
    1. 计算距离矩阵
    2. 计算距离矩阵每行(列)之和
    3. 找到最小的和
* 方法二
    1. 计算mean
    2. 找到距离mean最近的数据点



K-medoids算法可以由一下5个步骤完成:
1. 把所有数据点划分为k个非空子集
2. 计算每个子集的中心点(mean)
3. 把离中心点最近的数据点(medoid)作为该子集的实际中心点
4. 将所有数据点重新划分(划分到离该数据点最近的中心点所表示的子集)
5. 重复步骤2，直到中心点不发生变化

## 谱聚类

> • 带权无向图G=<V,E>
> • V为图的顶点集(数据点)
> • E为图的边集(相似度，和距离成反比)
> • 目标是将图G切分为多个不相交的子图，使子图内相似度较高，子图间相似度较低。

特点
- 相对来说更复杂缓慢
- 能在任意形状的样本空间上得到较好的聚类效果（如示例中的同心圆情况）
- 利用了图论的思想和其他的聚类算法（例如k-means），优化了聚类效果



目标1：找到最小切分$\min cut(A,B)$，其中$cut(A, B) = \sum_{i \in A, j \in B}w_{ij}$

目标2：最大化类内连接相似度$\max(assoc(A,A)+assoc(B,B)$，其中$assoc(A,A) = \sum_{i \in A, j \in A}w_{ij}$

目标3：最小化$\min Ncut(A,B)$，其中Normalized-cut $Ncut(A,B)=\frac{cut(A,B)}{assoc(A,V)}+\frac{cut(A,B)}{assoc(B,V)}$

推论：

* $cut(A,B) = assoc(A,V)-assoc(A,A) = assoc(B,V)-assoc(B,B)$
* $Ncut(A,B)=\frac{assoc(A,V)-assoc(A,A)}{assoc(A,V)}+\frac{assoc(B,V)-assoc(B,B)}{assoc(B,V)} = 2 - (\frac{assoc(A,A)}{assoc(A,V)}+\frac{assoc(B,B)}{assoc(B,V)}) = 2-Nassoc(A,B)$

**目标函数**？？？？

**实现思路**：

1. 数据准备，生成图的邻接矩阵
2. 归一化普拉斯矩阵；
    * $D^{-1}L$
3. 生成最小的k个特征值和对应的特征向量
    * 先获得所有的特征值和特征向量然后排序取得最小的k个
    * 获得N*k的特征矩阵
4. 将特征向量kmeans聚类(少量的特征向量)
    * 将矩阵每一行作为一个k维的样本，共N个样本，调用K-means方法

# 分类

## 回归

## 

## 神经网络

### 感知机

只有0和1
$$
y = 
\begin{cases}
    0 & w_1x_1 + w_2x_2 \leqslant \theta
    \\
    1 & w_1x_1 + w_2x_2 > \theta
\end{cases}
$$

令$b=-\theta$，则有$S=w_1x_1 + w_2x_2 + b$，称$b$为偏置



感知机的缺陷，1961AI寒冬：一三象限，二四象限无法用直线边界划分