# jCache

## Cache Basis

**SRAM**

<img src="https://miro.medium.com/max/795/1*BE7wrx1HjlHiTBULXnAiUA.png">

[SRAM and DRAM (SDRAM). Everyone of us are familiar with RAMâ€¦ | by Arunkumar Krishnan | Medium](https://medium.com/@emailarunkumar/sram-and-dram-sdram-9b6d01f09eb7)



**Cache Locality**

* Temporal locality 
    * need the requested word again soon
* Spatial locality
    * likely need other data in the block soon



**Cache Miss**

Time required for cache miss depends on: 

* **Latency**: the time to retrieve the first word of the block (å¯»å€)
    * depends on queueing, cpu scheduling, dram scheduling, etcâ€¦
* **Bandwidth**: the time to retrieve the rest of this block
    * depends on how many data can be transmitted per time unit, so the more data can transmit per time unit, the faster to transmit a fixed amount of data.
* ä¼˜åŒ–æ—¶ä¸¤è€…åŸºæœ¬ä¸Šæ²¡æœ‰å…³ç³»



**Cache Miss Metrics**

* Memory stall cycles per mem access
    * the number of cycles during processor is stalled waiting for a mem access
* Miss rate
    * number of misses over 
    * number of accesses
* Miss penalty
    * the cost per miss (number of extra clock cycles to wait)



## Cache Performance

`CPU time = (CPU execution clock cycles + Memory-stall clock cycles) Ã— Clock cycle time`

è¯»æ“ä½œé˜»å¡çš„å‘¨æœŸï¼š`Read_stall_cycles = (Reads/Program) Ã— Read_miss_rate Ã— Read_miss_penalty`

å†™æ“ä½œé˜»å¡çš„å‘¨æœŸï¼š`Write_stall_cycles = [(Writes/Program) Ã— Write_miss_rate Ã— Write_miss_penalty] + Write_buffer_stalls` (å†™ç¼ºå¤±+å†™ç¼“å†²é˜»å¡)

å†™ç¼“å†²åŒºé˜»å¡ï¼š`Write_buffer_stalls`ï¼šä¸ªäººç†è§£åº”è¯¥æŒ‡çš„æ˜¯è¿ç»­å¤šæ¬¡å†™æ“ä½œä¸­ï¼Œä¸‹ä¸€æ¬¡å¾—ç­‰ä¸Šä¸€æ¬¡å†™å®Œæ‰å¯ä»¥å†™ã€‚å–å†³äºé¢‘ç‡å’Œwriteçš„æ—¶æœº(æ¯”å¦‚ä½ è¦æ˜¯èƒ½ä¿è¯memä»»æ„æ—¶åˆ»å†™ä¸€æ¬¡çš„æ—¶é—´ä¹‹å†…ä¸ä¼šé‡åˆ°å…¶ä»–çš„å†™è¦æ±‚ï¼Œé‚£å°±ä¸å­˜åœ¨è¿™ä¸ªé—®é¢˜)ï¼Œå› æ­¤æ²¡åŠæ³•é‡åŒ–è®¡ç®—ã€‚

å­˜å‚¨å™¨é˜»å¡æ—¶é’Ÿå‘¨æœŸæ•°ï¼š`Memory-stall clock cycles `

\\            `= (Read-stall cycles + Write-stall cycles)`

\\            `= (Memory_accesses/Program) Ã— Miss_rate Ã— Miss_penalty`(å¿½ç•¥å†™ç¼“å†²åŒºé˜»å¡ï¼Œå…±ç”¨è¯»å†™çš„MissRateå’ŒMissPenalty)

\\            `= (Instructions/Program) Ã— (Misses/Instruction) Ã— Miss_penalty`(è¿™ç¬¬ä¸€ä¸ªæ‹¬å·åº”è¯¥æ˜¯åªè€ƒè™‘å†…å­˜ä¸­çš„æŒ‡ä»¤)



**Example**

> a computer with CPI=1 when cache hit;
> 50% instructions are loads and stores;
> 2% miss rate, 25 cc miss penalty;

MemStallCycle = IC \* MemAccPerInst \* MissRate \* MissPenalty

\\                        = IC \* (1+0.5) \* 0.02 \* 025  ==(è¿™é‡Œ<u>1æ˜¯IF</u>ï¼Œ0.5æ˜¯DataAccess)==

\\                        = 0.75IC

## Block Placement

* DM
* FA
* SA

## Block Identification

Tag

## Replacement Strategy

* Random
* LRU
    * æ²¡æœ‰Beladyâ€™s Anomaly
    * ç®—blockNum=xxxçš„æ—¶å€™å¯ä»¥ç”¨stackå›¾ä¸€æ¬¡æ€§æå®š
    * è¿™éƒ¨åˆ†å»ºè®®ç›´æ¥çœ‹[OSçš„ç¬¬åç« çš„ç¬”è®°](../OS/10_VM.md)
* FIFO
* Psudo LRU
    * For each cache set, assign one bit per block/line
    * Upon accessing a block, turn on its bit
    * If all bits are turned on, reset them except for that of the most recently accessed block
    * For replacement, randomly choose one whose bit is turned off

## Write Strategy

### Write hit

* Write-back
    * info is written only to the block in the cache
    * to the main memory only when the modified cache block is replaced (use dirty bit)
* Write-through
    * info is written to both the block in the cache and to the block in the lower-level memory

### Write miss

* Write allocate
    * the block is allocated on a write miss, followed by a write hit
* Write around
    * write miss not affect the cache, the block is modified in memory
    * until the program tries to read the block

## Performance Cal

![](assets/cache_performance.png)

==æ³¨æ„==

* æœ€åä¸€ä¸ªå…¬å¼ï¼Œä¸è€ƒè™‘L1çš„hit

**Example**

> 16KB instr cache + 16KB data cache;
> or, 32KB unified cache;
>
> 36% data transfer instructions; (<u>load/store takes 1 extra cc on unified cache</u>)
> 1 CC hit; 200 CC miss penalty;
>
> For 1000 instructions: 3.82 inst miss on 16KB icache, 40.9 data miss on 16KB dcache; or 43.3 miss on unified cache
>
> Q1: split cache or unified cache has lower miss rate? 

MR_16i = 3.82/1000/1.0 = 0.004

MR_16d = 40.9/1000/0.36 = 0.114 (==æ­¤å¤„é™¤ä»¥0.36æ˜¯å› ä¸ºmissrate<u>æ˜¯ä»¥è®¿é—®æ¬¡æ•°è€Œä¸æ˜¯æŒ‡ä»¤æ•°ä¸ºåˆ†æ¯çš„</u>==ï¼Œä¸‹åŒ)

In all accesses, 1/(1+0.36) = 0.74 are inst accesses, therefore overall miss rate = 0.74\*0.004+0.26\*0.114=0.0326

MR_32u = 43.3/1000/1.36 = 0.0318

> Q2: average memory access time?

= inst_rate \* (hit_time + inst_mr \* mp) + ....

split = 0.74 \* (1 + 0.04 \* 200) + 0.26 \* (1 + 0.114 \* 200) = 7.52

unified = 0.74 \* (1 + 0.00318 \* 200) + 0.26 \* (1 + 1 + 0.0318 \* 200) = 7.52



**Ex.2.**

> 200-cc cache miss penalty;
> 1-cc instruction;
> 2% miss rate;
> 1.5 memory reference per instruction; 
> 30 cache misses per 1000 instructions;
>
> Cache impact on performance?

**with cache:**

* method1
    * CPU time = IC \* (CPI_normal + stall_clk/inst_num) \* clk_time<br />= IC \* (1 + (200 \* 30 / 1000)) \* clk_time = 7 \* ...
* method2
    * CPU time = IC \* (CPI_normal + mr \* accrss_num \* mp / inst_num) \* clk_time<br /> = 7 \* ...

**without cache:**

CPI_avg = 1.0 + 1.5 \* 200 = 301



**Ex.3.**

> 1.6 cc CPI, clock cycle time of 0.35 ns;
> 1.4 memory references per instruction;
> 128 KB cache, 64-byte block;
> for two-way set associative, 1.35 times of clock cycle time for selection mux; 
> 1-cc hit time; 65ns miss penalty
> miss rate: 2.1% - direct, 1.9% - set;
>
> Calculate average memory access time and then processor performance?

AMAT_1way = 0.35 + (0.021 \* 65) = 1.72ns

AMAT_2way = 0.35 \* 1.35 + (0.019 \* 65) = 1.71ns

## Optimization

### Causes of Miss Rates

* Compulsory: 
    * The very first access to a block cannot be in the cache, so the block must be brought into the cache. 
    * ä¹Ÿå«åšcold-start/first-reference misses;
        * å°½é‡è®©è¯»è¿›æ¥çš„ä¸€ä¸ªblocké‡Œçš„éƒ½æ˜¯æœ‰æ•ˆçš„
* Capacity
    * The cache cannot contain all the blocks needed during execution of a program
    * cache size limit;
        * ç”¨æ°å½“çš„æ˜ å°„æ–¹å¼
    * blocks discarded and later retrieved;
        * ä¼˜åŒ–æ›¿æ¢ç®—æ³•
* Conflict
    * If the block placement strategy is set associative or direct mapped, conflict misses (in addition to compulsory and capacity misses) will occur because a block may be discarded and later retrieved if too many blocks map to its set.
    * collision misses: associativity
        * direct mapped or set associative cache;
        * a block discarded and later retrieved in a set;

### Opt

#### Larger Block

* ğŸ‘Reduce compulsory misses
    * Leverage spatial locality
* ğŸ‘Increase conflict/capacity misses
    * Fewer blocks in the cache
    * Larger blocks increase miss penalty

#### Larger Cache

* ğŸ‘Reduce capacity misses
* ğŸ‘Increase hit time, cost, and power

#### Higher Associativity

(Higher set way)

* ğŸ‘Reduce conflict misses
* ğŸ‘Increase hit time
* cache rule of thumb:
    * a direct-mapped cache of size N has about the same miss rate as a two-way set associative cache of size N/2

#### Multilevel Cache

* ğŸ‘Reduce miss penalty
* Motivation
    * faster/smaller cache to keep pace with the speed of processorsï¼ˆä¸€çº§ç¼“å­˜ï¼‰
    * larger cache to overcome the widening gap between processor and main memï¼ˆäºŒçº§ç¼“å­˜ï¼‰
        * å¦‚æœç¬¬äºŒçº§ç¼“å­˜ä»…æ¯”ç¬¬ä¸€çº§å¤§ä¸€ç‚¹ç‚¹ï¼Œåˆ™miss penaltyåè€Œæ›´é«˜
* è®¡ç®—memory stallæ—¶ä¸åº”è€ƒè™‘L1 hit time



> Example
> 	1000 mem references -> 40 misses in L1 and 20 misses in L2;
> 	miss penalty from L2 is 200 cc;
> 	hit time of L2 is 10 cc;
> 	hit time of L1 is 1 cc;
> 	1.5 mem references per instruction;
> Q: 
> 1. various miss rates?
> 2. avg mem access time?
> 3. avg stall cycles per instruction?

1. various miss rates?
    	* L1
         * local = global = 40/1000 = 4%
    * L2
         * local: 20/40=50%; global: 20/1000=2%(global\_2=locaol\_1\*local_2)
2. avg mem access time?
     * average memory access time =Hit_time_L1 + Miss_rate_L1 x (Hit_time_L2 + Miss_rate_L2 x Miss_penalty_L2)
          =1 + 4% x (10 + 50% x 200)
          =5.4
3. avg stall cycles per instruction?
     * average stall cycles per instruction=Misses_per_instruction_L1 x Hit_time_L2 + Misses_per_instr_L2 x Miss_penalty_L2
          =(1.5x40/1000)x10+(1.5x20/1000)x200
          =6.6
     * ==L1 hit not considered in "stall"==

#### Prioritize read misses over writes

Motivation:

```assembly
# DM(blockNum=512), write through
sw r1, 0(r0)	# cache[0]=r1, write_buf=r1(not to mem)
lw r2, 512(r0)	# cache[0]=r2, write_buf=r1(not to mem)
lw r3, 0(r0)	# cache miss, read from mem, but r1 still in write_buf, ERROR
```

* Reduce miss penalty
* not simply stall read miss until write buffer empties, check the contents of write buffer, let the read miss continue <u>if no conflicts with write buffer</u> & memory system is available

#### Avoid address translation during indexing cache

* Cache addressing
    * virtual address â€“ virtual cache
    * physical address â€“ physical cache
* Processor/program â€“ virtual address
* Processor -> address translation -> Cache



![](assets/image-20201207112253484.png)

Requirements

* L1 cache
    * L1\_entry\_num = page\_entry\_num = 2^8^
    * å› ä¸ºcacheæ˜¯ç”¨phy addrå»ç¼–å€çš„ï¼Œæ‰€ä»¥ä¸èƒ½ç”¨VPNæ¥å¯»ï¼Œåªèƒ½ç”¨PPO=VPOä¸­çš„é«˜8ä½ä½œä¸ºL1_cacheçš„indexå…ˆå»è¯•ä¸€æ¬¡ï¼Œè¿™æ‰è¦æ±‚æœ‰ä¸Šé¢é‚£ä¸ªæ¡ä»¶
* L2 cache
    * L2æ—¶å·²ç»å¾—åˆ°PAäº†ï¼Œå› æ­¤å¯ä»¥æŠŠæ•´ä¸ªPAç”¨æ¥å¯»æ•°æ®